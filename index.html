<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LexiMark is a robust watermarking technique that embeds subtle synonym substitutions into text to enhance membership-verification for LLM pre-training data.">
  <meta name="keywords"
        content="LexiMark, watermarking, membership-inference attack, pre-training data detection, large language models, privacy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LexiMark — Robust Watermarking for LLM Pre-training Verification</title>

  <!-- ⚠ OPTIONAL: add your own Google-Analytics tag or remove this block entirely. -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-X"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);} gtag('js', new Date());
    gtag('config', 'UA-XXXXXXX-X');
  </script>
  -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- ░░░ Navbar ░░░ -->
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sagivantebi.com">
        <span class="icon"><i class="fas fa-home"></i></span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">More&nbsp;Research</a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2401.09075">The Risk of Customized GPTs&nbsp;- 2024</a>
          <a class="navbar-item" href="https://arxiv.org/abs/2501.08454">Tag&Tab&nbsp;- 2025</a>
          <a class="navbar-item" href="#">TabMIA&nbsp;- 2025</a>
        </div>
      </div>
    </div>
  </div>
</nav> -->

<!-- ░░░ Hero ░░░ -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            LexiMark: Robust Watermarking via<br>
            Lexical Substitutions to Enhance Membership<br>
            Verification of an LLM’s Textual Training Data
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://scholar.google.com/citations?user=MB6doTkAAAAJ">Eyal German</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=j_WBtHIAAAAJ">Sagiv Antebi</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=9zI29XgAAAAJ">Edan Habler</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=k-J7GfgAAAAJ">Asaf Shabtai</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=ruZDm9QAAAAJ">Yuval Elovici</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Ben-Gurion University of the Negev</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.14474" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span>
              <!-- Code repo -->
              <span class="link-block">
                <a href="https://github.com/eyalgerman/LexiMark" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
              <!-- PDF placeholder
              <span class="link-block">
                <a href="./static/pdfs/leximark_paper.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ░░░ Teaser ░░░ -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <figure class="image">
      <img src="./static/images/leximark_method.jpg"
          alt="LexiMark watermark-and-MIA workflow with key highlights"
          style="max-width:100%; height:auto;">
    </figure>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">LexiMark</span> embeds high-entropy synonym substitutions that LLMs memorize&nbsp;—
        enabling reliable post-training membership verification without harming semantics.
      </h2> -->
    </div>
  </div>
</section>

<!-- ░░░ Results carousel ░░░ -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <!-- Example item; duplicate or replace as needed -->
        <div class="item">
          <img src="./static/images/placeholder_result.jpg" alt="Example result" />
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ░░░ Abstract ░░░ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner’s consent.
            Verifying whether a specific LLM was trained on particular data instances—or on an entire dataset—is
            extremely challenging. <em>Dataset watermarking</em> addresses this by embedding identifiable modifications
            in training data to detect unauthorized use. However, existing methods often lack stealth, making them
            relatively easy to detect and remove.
          </p>

          <p>
            We propose <strong>LexiMark</strong>, a novel watermarking technique for text and documents that embeds synonym
            substitutions for carefully selected high-entropy words. The substitutions boost an LLM’s memorization of
            the watermarked text without altering its semantic integrity. Consequently, the watermark blends
            seamlessly into the text—no visible markers—while remaining resistant to automated or manual removal.
          </p>

          <p>
            We evaluate LexiMark on baseline datasets from recent studies and seven open-source models:
            LLaMA-1&nbsp;7B, LLaMA-3&nbsp;8B, Mistral&nbsp;7B, Pythia&nbsp;6.9B, and three smaller Pythia variants
            (160 M, 410 M, 1 B). Our experiments cover continued-pretraining and fine-tuning scenarios. LexiMark
            consistently boosts AUROC scores over prior methods, demonstrating its effectiveness in reliably detecting
            whether unauthorized watermarked data was used during LLM training.
          </p>        </div>
      </div>
    </div>
  </div>
</section>

<!-- ░░░ Method ░░░ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p><strong>LexiMark</strong> operates in two phases:</p>
          <ul>
            <li><strong>Watermark Embedding.</strong> For every sentence we (i) rank words by
                corpus entropy, (ii) select the top-K, and (iii) substitute each with a
                synonym that yields even higher entropy while retaining syntactic
                correctness. Substitutions are blocked on stop-words and named entities.</li>
            <li><strong>Watermark Detection.</strong> After the suspect model is trained, we run any
                standard black-box MIA (e.g.&nbsp;Min-K++ 20%) on the <em>watermarked</em> tokens only.
                The resulting likelihood gap yields state-of-the-art AUROC with as few as
                six records.</li>
          </ul>
          <p>The approach is <em>stealthy</em> (no unnatural glyphs), <em>robust</em> (survives post-training
             and mild edits) and <em>portable</em> (no auxiliary models required).</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ░░░ Results tables ░░░ -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Fine-tune LLMs</h2>
<table class="table is-striped is-hoverable is-fullwidth">
  <caption class="has-text-weight-semibold">
    Comparison of watermarking and non-watermarking methods on various datasets and models
    (AUROC / TPR&nbsp;@&nbsp;FPR = 5&nbsp;%).<br>
    <em>k = 5</em>, concatenation synonym selection, Min-K++ 20 % MIA.
    Bold values indicate the best score for each dataset–model pair.
  </caption>

  <!-- Column headers -->
  <thead>
    <tr>
      <th rowspan="2">Dataset</th>
      <th rowspan="2">Metric</th>
      <th colspan="2">Pythia&nbsp;6.9 B</th>
      <th colspan="2">LLaMA-1&nbsp;7 B</th>
      <th colspan="2">LLaMA-3&nbsp;8 B</th>
      <th colspan="2">Mistral&nbsp;7 B</th>
    </tr>
    <tr>
      <th>None</th><th>LexiMark</th>
      <th>None</th><th>LexiMark</th>
      <th>None</th><th>LexiMark</th>
      <th>None</th><th>LexiMark</th>
    </tr>
  </thead>

  <tbody>
    <!-- BookMIA -->
    <tr>
      <td rowspan="2">BookMIA</td>
      <td>AUROC</td>
      <td>69.1</td><td><strong>94.8</strong></td>
      <td>73.2</td><td><strong>95.9</strong></td>
      <td>79.0</td><td><strong>96.9</strong></td>
      <td>84.7</td><td><strong>96.7</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5&nbsp;%&nbsp;FPR</td>
      <td>13.5</td><td><strong>79.1</strong></td>
      <td>18.3</td><td><strong>84.3</strong></td>
      <td>24.3</td><td><strong>84.4</strong></td>
      <td>30.2</td><td><strong>90.9</strong></td>
    </tr>

    <!-- Enron Emails -->
    <tr>
      <td rowspan="2">Enron Emails</td>
      <td>AUROC</td>
      <td>65.6</td><td><strong>72.3</strong></td>
      <td>65.6</td><td><strong>69.8</strong></td>
      <td>71.3</td><td><strong>75.3</strong></td>
      <td>78.1</td><td><strong>81.6</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5&nbsp;%&nbsp;FPR</td>
      <td>11.0</td><td><strong>23.8</strong></td>
      <td>11.0</td><td><strong>19.4</strong></td>
      <td>12.4</td><td><strong>21.3</strong></td>
      <td>27.7</td><td><strong>31.2</strong></td>
    </tr>

    <!-- PubMed Abstracts -->
    <tr>
      <td rowspan="2">PubMed Abstracts</td>
      <td>AUROC</td>
      <td>68.7</td><td><strong>76.0</strong></td>
      <td>72.2</td><td><strong>80.7</strong></td>
      <td>78.4</td><td><strong>83.3</strong></td>
      <td>83.8</td><td><strong>88.7</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5&nbsp;%&nbsp;FPR</td>
      <td>17.9</td><td><strong>25.0</strong></td>
      <td>23.6</td><td><strong>35.0</strong></td>
      <td>35.4</td><td><strong>41.5</strong></td>
      <td>48.4</td><td><strong>58.4</strong></td>
    </tr>

    <!-- Wikipedia (en) -->
    <tr>
      <td rowspan="2">Wikipedia&nbsp;(en)</td>
      <td>AUROC</td>
      <td>65.5</td><td><strong>74.5</strong></td>
      <td>63.1</td><td><strong>73.0</strong></td>
      <td>70.8</td><td><strong>78.9</strong></td>
      <td>77.2</td><td><strong>84.6</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5&nbsp;%&nbsp;FPR</td>
      <td>10.2</td><td><strong>16.6</strong></td>
      <td>12.4</td><td><strong>19.7</strong></td>
      <td>14.2</td><td><strong>22.8</strong></td>
      <td>18.1</td><td><strong>31.7</strong></td>
    </tr>

    <!-- PILE-FreeLaw -->
    <tr>
      <td rowspan="2">PILE&nbsp;– FreeLaw</td>
      <td>AUROC</td>
      <td>67.7</td><td><strong>83.3</strong></td>
      <td>57.2</td><td><strong>61.6</strong></td>
      <td>70.9</td><td><strong>87.0</strong></td>
      <td>80.1</td><td><strong>92.0</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5&nbsp;%&nbsp;FPR</td>
      <td>10.0</td><td><strong>37.0</strong></td>
      <td>9.8 </td><td><strong>23.7</strong></td>
      <td>11.8</td><td><strong>42.6</strong></td>
      <td>18.5</td><td><strong>67.1</strong></td>
    </tr>

    <!-- USPTO Backgrounds -->
    <tr>
      <td rowspan="2">USPTO Backgrounds</td>
      <td>AUROC</td>
      <td>63.4</td><td><strong>76.1</strong></td>
      <td>65.0</td><td><strong>78.5</strong></td>
      <td>72.4</td><td><strong>82.5</strong></td>
      <td>82.0</td><td><strong>89.8</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5&nbsp;%&nbsp;FPR</td>
      <td>9.2 </td><td><strong>22.7</strong></td>
      <td>14.5</td><td><strong>28.3</strong></td>
      <td>21.1</td><td><strong>35.2</strong></td>
      <td>39.6</td><td><strong>60.6</strong></td>
    </tr>
  </tbody>
</table>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Continued Pretraining</h2>
<!-- ░░░ Fine-tuned models results table ░░░ -->
<table class="table is-striped is-hoverable is-fullwidth">
  <caption class="has-text-weight-semibold">
    Comparison of watermarking and non-watermarking methods on various datasets and models
    (AUROC / TPR&nbsp;@&nbsp;FPR = 5 %).<br>
    <em>k = 5</em>, concatenation-based synonym selection, Min-K++ 20 % MIA.
    Bold values mark the best score per dataset–model pair.
  </caption>

  <!-- Column headers -->
  <thead>
    <tr>
      <th rowspan="2">Dataset</th>
      <th rowspan="2">Metric</th>
      <th colspan="2">Pythia&nbsp;160 M</th>
      <th colspan="2">Pythia&nbsp;410 M</th>
      <th colspan="2">Pythia&nbsp;1 B</th>
    </tr>
    <tr>
      <th>None</th><th>LexiMark</th>
      <th>None</th><th>LexiMark</th>
      <th>None</th><th>LexiMark</th>
    </tr>
  </thead>

  <tbody>
    <!-- BookMIA -->
    <tr>
      <td rowspan="2">BookMIA</td>
      <td>AUROC</td>
      <td>77.5</td><td><strong>95.0</strong></td>
      <td>87.3</td><td><strong>97.0</strong></td>
      <td>88.1</td><td><strong>96.2</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5 % FPR</td>
      <td>18.0</td><td><strong>89.5</strong></td>
      <td>25.0</td><td><strong>95.9</strong></td>
      <td>24.5</td><td><strong>95.0</strong></td>
    </tr>

    <!-- Enron Emails -->
    <tr>
      <td rowspan="2">Enron Emails</td>
      <td>AUROC</td>
      <td>79.1</td><td><strong>85.2</strong></td>
      <td>84.6</td><td><strong>87.6</strong></td>
      <td>85.8</td><td><strong>89.0</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5 % FPR</td>
      <td>26.8</td><td><strong>51.3</strong></td>
      <td>31.0</td><td><strong>59.2</strong></td>
      <td>48.0</td><td><strong>68.4</strong></td>
    </tr>

    <!-- PubMed Abstracts -->
    <tr>
      <td rowspan="2">PubMed Abstracts</td>
      <td>AUROC</td>
      <td>69.9</td><td><strong>77.9</strong></td>
      <td>86.5</td><td><strong>89.0</strong></td>
      <td>93.8</td><td><strong>96.5</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5 % FPR</td>
      <td>17.9</td><td><strong>26.8</strong></td>
      <td>52.9</td><td><strong>60.2</strong></td>
      <td>82.7</td><td><strong>89.8</strong></td>
    </tr>

    <!-- Wikipedia -->
    <tr>
      <td rowspan="2">Wikipedia&nbsp;(en)</td>
      <td>AUROC</td>
      <td>68.4</td><td><strong>74.5</strong></td>
      <td>76.8</td><td><strong>84.5</strong></td>
      <td>80.2</td><td><strong>87.9</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5 % FPR</td>
      <td>10.0</td><td><strong>17.0</strong></td>
      <td>18.1</td><td><strong>37.9</strong></td>
      <td>33.4</td><td><strong>57.1</strong></td>
    </tr>

    <!-- PILE-FreeLaw -->
    <tr>
      <td rowspan="2">PILE - FreeLaw</td>
      <td>AUROC</td>
      <td>67.2</td><td><strong>79.8</strong></td>
      <td>73.9</td><td><strong>87.1</strong></td>
      <td>78.1</td><td><strong>91.4</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5 % FPR</td>
      <td>13.5</td><td><strong>34.5</strong></td>
      <td>18.8</td><td><strong>46.9</strong></td>
      <td>23.4</td><td><strong>64.3</strong></td>
    </tr>

    <!-- USPTO Backgrounds -->
    <tr>
      <td rowspan="2">USPTO Backgrounds</td>
      <td>AUROC</td>
      <td>69.5</td><td><strong>79.4</strong></td>
      <td>80.5</td><td><strong>89.8</strong></td>
      <td>83.5</td><td><strong>92.0</strong></td>
    </tr>
    <tr>
      <td>TPR&nbsp;@&nbsp;5 % FPR</td>
      <td>17.4</td><td><strong>29.5</strong></td>
      <td>41.5</td><td><strong>61.2</strong></td>
      <td>54.1</td><td><strong>73.2</strong></td>
    </tr>
  </tbody>
</table>

      </div>
    </div>
  </div>
</section>


<!-- ░░░ BibTeX ░░░ -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@misc{german2025leximarkrobustwatermarkinglexical,
  title        = {LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data},
  author       = {Eyal German and Sagiv Antebi and Edan Habler and Asaf Shabtai and Yuval Elovici},
  year         = {2025},
  eprint       = {2506.14474},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2506.14474}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            You are welcome to re-use the
            <a href="https://github.com/nerfies/nerfies.github.io">original source
            code</a>; please keep a link back to this page and remove the dummy
            analytics snippet above.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
